---
title: Terminology
---

This page contains definitions for commonly used terms in the OpenProblems project.

## OpenProblems
Consists of dataset loaders and tasks

## Dataset loader

## Task
Consists of a dataset processor, methods, metrics and baseline methods.

### Dataset processor

### Method

### Baseline method
A baseline method (or control method) is a method against which other methods are compared. It serves as a starting point to test the relative accuracy of new methods in the task, and also as a quality control for the metrics defined in the task. A baseline method can either be a positive control or a negative control.

A positive control is a method where the expected results are known, thus resulting in the best possible value for any metric outcome measure. A negative control is a simple, naive, or random method that does not rely on any sophisticated techniques or domain knowledge.

The positive control and negative control methods set a maximum and minimum threshold for performance, so any new method should perform better than the negative control methods and worse than the positive control method.

### Metrics


:::{.content-hidden}
The project repository is structured as follows:

* `resources_test`: Datasets for testing components. This folder can be obtained by running step 3.
* `src`: Source files for each component in the pipeline.
  - `common`: Common processing components.
  - `datasets`: Components and pipelines for building the 'Common datasets'
  - `label_projection`: Source files related to the 'Label projection' task.
  - `...`: Other tasks.
* `target`: Artifacts built from the components in `src/` by running `viash ns build`.
  - `docker`: Bash executables which can be used from a terminal.
  - `nextflow`: Nextflow modules which can be used as a standalone pipeline or as part of a bigger pipeline.

Detailed overview of a task folder (e.g. `src/label_projection`):

* `api`: Specifications for the components and data files in this task.
* `control_methods`: Control methods which serve as quality control checks for the benchmark.
* `methods`: Label projection method components.
* `metrics`: Label projection metric components.
* `process_dataset`: A component that splits a common dataset into a solution and training dataset.
* `resources_scripts`: The scripts needed to run the benchmark.
* `resources_test_scripts`: The scripts needed to generate the test resources, which are required for unit testing.
* `workflows`: The benchmarking workflow.
:::